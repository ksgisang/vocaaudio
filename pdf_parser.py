"""
PDFì—ì„œ ì˜ì–´ ë‹¨ì–´ì™€ í•œêµ­ì–´ ëœ»ì„ ì¶”ì¶œí•˜ëŠ” ëª¨ë“ˆ
Google Cloud Vision API ì§€ì› ì¶”ê°€ (ë†’ì€ ì¸ì‹ë¥ )
"""
import re
import io
import os
from dataclasses import dataclass
from typing import List, Optional

# PyMuPDF (ì„ íƒì )
try:
    import fitz
    HAS_FITZ = True
except ImportError:
    HAS_FITZ = False

# OCR ê´€ë ¨ - pytesseract (fallback)
try:
    import pytesseract
    from PIL import Image, ImageEnhance
    HAS_OCR = True
except ImportError:
    HAS_OCR = False

# Google Cloud Vision API
try:
    from google.cloud import vision
    HAS_CLOUD_VISION = True
except ImportError:
    HAS_CLOUD_VISION = False

# Anthropic Claude Vision API
try:
    import anthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False

# Google Gemini Vision API (ë¬´ë£Œ) - ìƒˆë¡œìš´ íŒ¨í‚¤ì§€
try:
    from google import genai
    from google.genai import types
    HAS_GEMINI = True
except ImportError:
    HAS_GEMINI = False


def detect_and_fix_orientation(img: 'Image.Image') -> 'Image.Image':
    """
    ì´ë¯¸ì§€ì˜ ë°©í–¥ì„ ê°ì§€í•˜ê³  í•„ìš”ì‹œ íšŒì „
    ê±°ê¾¸ë¡œ ëœ ì´ë¯¸ì§€(180ë„)ë¥¼ ìë™ìœ¼ë¡œ ë°”ë¡œì¡ìŒ
    """
    if not HAS_OCR:
        return img

    try:
        # Tesseract OSDë¡œ ë°©í–¥ ê°ì§€
        osd = pytesseract.image_to_osd(img, output_type=pytesseract.Output.DICT)
        rotation = osd.get('rotate', 0)

        if rotation != 0:
            # ì´ë¯¸ì§€ íšŒì „ (Tesseractê°€ ì•Œë ¤ì¤€ ê°ë„ë§Œí¼)
            img = img.rotate(-rotation, expand=True)

        return img
    except Exception:
        # OSD ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë°©ë²•: í…ìŠ¤íŠ¸ ë°©í–¥ íœ´ë¦¬ìŠ¤í‹± ì²´í¬
        # ì›ë³¸ê³¼ 180ë„ íšŒì „ ì´ë¯¸ì§€ ëª¨ë‘ OCR ì‹œë„í•´ì„œ ë” ë‚˜ì€ ê²°ê³¼ ì„ íƒ
        try:
            # ì‘ì€ ìƒ˜í”Œë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
            test_img = img.copy()
            test_img.thumbnail((800, 800))

            # ì›ë³¸ OCR
            text_original = pytesseract.image_to_string(test_img, lang='eng', config='--psm 6')
            # ì˜ì–´ ë‹¨ì–´ ê°œìˆ˜ ì„¸ê¸°
            english_words_original = len(re.findall(r'\b[a-zA-Z]{2,}\b', text_original))

            # 180ë„ íšŒì „ í›„ OCR
            test_rotated = test_img.rotate(180)
            text_rotated = pytesseract.image_to_string(test_rotated, lang='eng', config='--psm 6')
            english_words_rotated = len(re.findall(r'\b[a-zA-Z]{2,}\b', text_rotated))

            # ë” ë§ì€ ì˜ì–´ ë‹¨ì–´ê°€ ì¸ì‹ëœ ë°©í–¥ ì„ íƒ
            if english_words_rotated > english_words_original * 1.5:
                return img.rotate(180)

        except Exception:
            pass

        return img


@dataclass
class VocabItem:
    """ì–´íœ˜ í•­ëª©"""
    number: int
    word: str
    meaning: str
    pos: Optional[str] = None  # í’ˆì‚¬ (ë‚˜ì¤‘ì— ì‚¬ìš© ê°€ëŠ¥)


def extract_text_from_pdf(pdf_path: str) -> str:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    doc.close()
    return text


def extract_text_with_ocr(pdf_path: str, progress_callback=None) -> str:
    """OCRì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤ìº” PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not HAS_OCR:
        raise ImportError("OCR ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ pytesseractì™€ pillowë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")

    doc = fitz.open(pdf_path)
    text = ""
    total_pages = len(doc)

    for i, page in enumerate(doc):
        if progress_callback:
            progress_callback(i + 1, total_pages)

        # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (í•´ìƒë„ 300 DPI)
        mat = fitz.Matrix(300/72, 300/72)
        pix = page.get_pixmap(matrix=mat)

        # PIL Imageë¡œ ë³€í™˜
        img_data = pix.tobytes("png")
        img = Image.open(io.BytesIO(img_data))

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬: ê·¸ë ˆì´ìŠ¤ì¼€ì¼ + ì´ì§„í™”
        img = img.convert('L')  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
        # ì´ì§„í™” (threshold)
        threshold = 180
        img = img.point(lambda x: 255 if x > threshold else 0, '1')
        img = img.convert('L')  # ë‹¤ì‹œ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ

        # OCR ìˆ˜í–‰ (ì˜ì–´ + í•œêµ­ì–´, PSM 6: ë‹¨ì¼ ë¸”ë¡)
        custom_config = r'--psm 6'
        page_text = pytesseract.image_to_string(img, lang='eng+kor', config=custom_config)
        text += page_text + "\n"

    doc.close()
    return text


def extract_vocab_with_ocr_table(pdf_path: str, progress_callback=None, rotate: int = 0) -> List['VocabItem']:
    """
    í‘œ í˜•íƒœì˜ ìŠ¤ìº” PDFì—ì„œ ì§ì ‘ ì–´íœ˜ ì¶”ì¶œ
    ê° í˜ì´ì§€ë¥¼ ì™¼ìª½/ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    ìë™ìœ¼ë¡œ ì´ë¯¸ì§€ ë°©í–¥ ê°ì§€ ë° íšŒì „

    Args:
        rotate: ìˆ˜ë™ íšŒì „ ê°ë„ (0, 90, 180, 270). 0ì´ë©´ ìë™ ê°ì§€
    """
    if not HAS_OCR:
        raise ImportError("OCR ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ pytesseractì™€ pillowë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")

    doc = fitz.open(pdf_path)
    vocab_list = []
    total_pages = len(doc)

    for page_idx, page in enumerate(doc):
        if progress_callback:
            progress_callback(page_idx + 1, total_pages)

        # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (í•´ìƒë„ 300 DPI)
        mat = fitz.Matrix(300/72, 300/72)
        pix = page.get_pixmap(matrix=mat)
        img_data = pix.tobytes("png")
        img = Image.open(io.BytesIO(img_data))

        # ğŸ”„ ì´ë¯¸ì§€ íšŒì „
        if rotate != 0:
            # ìˆ˜ë™ íšŒì „ ì§€ì •ë¨
            img = img.rotate(-rotate, expand=True)
        else:
            # ìë™ ë°©í–¥ ê°ì§€ ë° íšŒì „
            img = detect_and_fix_orientation(img)

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img = img.convert('L')
        threshold = 180
        img = img.point(lambda x: 255 if x > threshold else 0, '1')
        img = img.convert('L')

        width, height = img.size

        # ì™¼ìª½ ì ˆë°˜ê³¼ ì˜¤ë¥¸ìª½ ì ˆë°˜ ë¶„ë¦¬ ì²˜ë¦¬
        for half_idx, (left, right) in enumerate([(0, width//2), (width//2, width)]):
            half_img = img.crop((left, 0, right, height))

            # OCR ìˆ˜í–‰
            custom_config = r'--psm 6'
            text = pytesseract.image_to_string(half_img, lang='eng+kor', config=custom_config)

            # í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
            items = parse_ocr_vocab_text(text)
            vocab_list.extend(items)

    doc.close()

    # ë²ˆí˜¸ë¡œ ì •ë ¬í•˜ê³  ì¤‘ë³µ ì œê±°
    vocab_list = sorted(vocab_list, key=lambda x: x.number)
    seen = set()
    unique_list = []
    for item in vocab_list:
        if item.number not in seen:
            seen.add(item.number)
            unique_list.append(item)

    return unique_list


def parse_ocr_vocab_text(text: str) -> List['VocabItem']:
    """OCR í…ìŠ¤íŠ¸ì—ì„œ ì–´íœ˜ í•­ëª© íŒŒì‹± (ë” ìœ ì—°í•œ íŒ¨í„´)"""
    vocab_list = []
    lines = text.split('\n')

    # íŒ¨í„´: ìˆ«ì + (ì²´í¬ë°•ìŠ¤) + ì˜ì–´ë‹¨ì–´
    # ë‹¤ìŒ ì¤„: í’ˆì‚¬. í•œêµ­ì–´ëœ»
    i = 0
    while i < len(lines):
        line = lines[i].strip()

        # ë²ˆí˜¸ì™€ ì˜ì–´ ë‹¨ì–´ ì°¾ê¸°
        # ì˜ˆ: "51 O empty" ë˜ëŠ” "51 â–¡ empty" ë˜ëŠ” "51 empty"
        match = re.match(r'^(\d+)\s*[Oâ–¡â˜\[\]oO]?\s*([a-zA-Z][a-zA-Z\-]*)', line)
        if match:
            number = int(match.group(1))
            word = match.group(2).strip()

            # ë‹¤ìŒ ì¤„ì—ì„œ ëœ» ì°¾ê¸°
            meaning = ""
            if i + 1 < len(lines):
                next_line = lines[i + 1].strip()
                # í•œê¸€ì´ í¬í•¨ëœ ì¤„
                if re.search(r'[ê°€-í£]', next_line):
                    # í’ˆì‚¬ ì œê±°
                    meaning = re.sub(r'^[avn]\.\s*', '', next_line)
                    meaning = re.sub(r'\s+[avn]\.\s*', ', ', meaning)
                    meaning = ' '.join(meaning.split())
                    i += 1

            # ê°™ì€ ì¤„ì— ëœ»ì´ ìˆì„ ìˆ˜ë„ ìˆìŒ
            if not meaning:
                rest = line[match.end():].strip()
                if re.search(r'[ê°€-í£]', rest):
                    meaning = re.sub(r'^[avn]\.\s*', '', rest)
                    meaning = re.sub(r'\s+[avn]\.\s*', ', ', meaning)
                    meaning = ' '.join(meaning.split())

            if word and len(word) > 1 and meaning:
                vocab_list.append(VocabItem(
                    number=number,
                    word=word,
                    meaning=meaning
                ))

        i += 1

    return vocab_list


def parse_vocab_table(text: str) -> List[VocabItem]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì–´íœ˜ í•­ëª© íŒŒì‹±
    í˜•ì‹: ë²ˆí˜¸ â–¡ ì˜ì–´ë‹¨ì–´ í’ˆì‚¬. í•œêµ­ì–´ëœ»
    """
    vocab_list = []

    # ì •ê·œì‹ íŒ¨í„´: ë²ˆí˜¸, ì²´í¬ë°•ìŠ¤, ì˜ì–´ë‹¨ì–´, í’ˆì‚¬+ëœ»
    # ì˜ˆ: "51 â–¡ empty a. ë¹„ì–´ ìˆëŠ” v. ë¹„ìš°ë‹¤"
    pattern = r'(\d+)\s*[â–¡â˜\[\]]\s*([a-zA-Z\-]+)\s+(.+?)(?=\d+\s*[â–¡â˜\[\]]|$)'

    matches = re.findall(pattern, text, re.DOTALL)

    for match in matches:
        number = int(match[0])
        word = match[1].strip()
        meaning_raw = match[2].strip()

        # í’ˆì‚¬ ì œê±°í•˜ê³  ëœ»ë§Œ ì¶”ì¶œ
        # í’ˆì‚¬ íŒ¨í„´: a. v. n. ad. ë“±
        meaning_clean = re.sub(r'\b[avn](?:d)?\.?\s*', '', meaning_raw)
        # ì—¬ëŸ¬ ì¤„/ê³µë°± ì •ë¦¬
        meaning_clean = ' '.join(meaning_clean.split())
        # ê´„í˜¸ ì•ˆ ë‚´ìš© ìœ ì§€, ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì—¬ëŸ¬ ëœ» ìœ ì§€
        meaning_clean = meaning_clean.strip()

        # í’ˆì‚¬ ì¶”ì¶œ (ë‚˜ì¤‘ì— ì‚¬ìš© ê°€ëŠ¥)
        pos_matches = re.findall(r'\b([avn](?:d)?)\.\s*', meaning_raw)
        pos = ', '.join(pos_matches) if pos_matches else None

        if word and meaning_clean:
            vocab_list.append(VocabItem(
                number=number,
                word=word,
                meaning=meaning_clean,
                pos=pos
            ))

    return vocab_list


def extract_vocab_from_pdf(pdf_path: str) -> List[VocabItem]:
    """PDFì—ì„œ ì–´íœ˜ ëª©ë¡ ì¶”ì¶œ (ë©”ì¸ í•¨ìˆ˜)"""
    text = extract_text_from_pdf(pdf_path)
    return parse_vocab_table(text)


# ëŒ€ì²´ íŒŒì‹± ë°©ë²• (í‘œ êµ¬ì¡°ê°€ ì˜ ì•ˆ ì½í ê²½ìš°)
def parse_vocab_simple(text: str) -> List[VocabItem]:
    """
    ê°„ë‹¨í•œ íŒŒì‹± ë°©ë²•
    OCR í…ìŠ¤íŠ¸ì—ì„œ ë²ˆí˜¸, ë‹¨ì–´, ëœ»ì„ ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì§€ì›)
    """
    vocab_list = []

    # ë°©ë²• 1: "ë²ˆí˜¸ â–¡ ë‹¨ì–´" + "í’ˆì‚¬. ëœ»" íŒ¨í„´ (2ì¤„ í˜•ì‹)
    # ì˜ˆ: "301 â–¡ heal" ë‹¤ìŒ ì¤„ "v. ì¹˜ë£Œí•˜ë‹¤"
    pattern_two_line = re.findall(
        r'(\d+)\s*[â–¡â˜\[\]O]?\s*([a-zA-Z][a-zA-Z\-]*(?:\s*\([^)]+\))?)\s*\n\s*([avn]\.?\s*[ê°€-í£].*?)(?=\n\d+|\n*$)',
        text, re.MULTILINE
    )
    if pattern_two_line:
        for match in pattern_two_line:
            number = int(match[0])
            word = match[1].strip()
            meaning = match[2].strip()
            # í’ˆì‚¬ ì œê±°
            meaning = re.sub(r'^[avn]\.\s*', '', meaning)
            if word and meaning:
                vocab_list.append(VocabItem(number=number, word=word, meaning=meaning))
        if vocab_list:
            return sorted(vocab_list, key=lambda x: x.number)

    # ë°©ë²• 2: "ë²ˆí˜¸ ë‹¨ì–´ í’ˆì‚¬.ëœ»" í•œ ì¤„ íŒ¨í„´
    # ì˜ˆ: "301 heal v. ì¹˜ë£Œí•˜ë‹¤"
    pattern_one_line = re.findall(
        r'(\d+)\s*[â–¡â˜\[\]O]?\s*([a-zA-Z][a-zA-Z\-]*(?:\s*\([^)]+\))?)\s+([avn]\.\s*[ê°€-í£][^\n]*)',
        text
    )
    if pattern_one_line:
        for match in pattern_one_line:
            number = int(match[0])
            word = match[1].strip()
            meaning = match[2].strip()
            meaning = re.sub(r'^[avn]\.\s*', '', meaning)
            if word and meaning:
                vocab_list.append(VocabItem(number=number, word=word, meaning=meaning))
        if vocab_list:
            return sorted(vocab_list, key=lambda x: x.number)

    # ë°©ë²• 3: ì¤„ ë‹¨ìœ„ ìƒíƒœ ê¸°ë°˜ íŒŒì‹± (fallback)
    lines = [line.strip() for line in text.split('\n') if line.strip()]

    current_number = None
    current_word = None

    def save_item(number, word, meaning):
        nonlocal vocab_list
        if number is None:
            number = len(vocab_list) + 1
        if not meaning:
            meaning = "[ëœ» ì…ë ¥ í•„ìš”]"
        vocab_list.append(VocabItem(number=number, word=word, meaning=meaning))

    for line in lines:
        # í•œ ì¤„ì— ë²ˆí˜¸ + ë‹¨ì–´ + ëœ»ì´ ëª¨ë‘ ìˆëŠ” ê²½ìš°
        full_match = re.match(r'^(\d+)\s*[â–¡â˜\[\]O]?\s*([a-zA-Z][a-zA-Z\-]*)\s+(.+)$', line)
        if full_match:
            number = int(full_match.group(1))
            word = full_match.group(2).strip()
            meaning = full_match.group(3).strip()
            # í’ˆì‚¬ ì œê±°
            meaning = re.sub(r'^[avn]\.\s*', '', meaning)
            if re.search(r'[ê°€-í£]', meaning):
                save_item(number, word, meaning)
                current_number = None
                current_word = None
                continue

        # ë²ˆí˜¸ + ë‹¨ì–´ë§Œ ìˆëŠ” ì¤„
        num_word_match = re.match(r'^(\d+)\s*[â–¡â˜\[\]O]?\s*([a-zA-Z][a-zA-Z\-]*)$', line)
        if num_word_match:
            if current_word:
                save_item(current_number, current_word, "")
            current_number = int(num_word_match.group(1))
            current_word = num_word_match.group(2).strip()
            continue

        # ëœ»ë§Œ ìˆëŠ” ì¤„ (í•œê¸€ í¬í•¨)
        if re.search(r'[ê°€-í£]', line) and current_word:
            meaning = re.sub(r'^[avn]\.\s*', '', line)
            meaning = ' '.join(meaning.split())
            save_item(current_number, current_word, meaning)
            current_word = None
            current_number = None
            continue

        # ì˜ë‹¨ì–´ë§Œ ìˆëŠ” ì¤„
        if re.match(r'^[a-zA-Z][a-zA-Z\-]*$', line):
            if current_word:
                save_item(current_number, current_word, "")
            current_word = line.strip()
            continue

    if current_word:
        save_item(current_number, current_word, "")

    return vocab_list


def load_vocab_from_text(file_path: str) -> List[VocabItem]:
    """
    í…ìŠ¤íŠ¸/CSV íŒŒì¼ì—ì„œ ì–´íœ˜ ë¡œë“œ
    í˜•ì‹: ë²ˆí˜¸,ë‹¨ì–´,ëœ» ë˜ëŠ” ë‹¨ì–´,ëœ»
    """
    # íŒŒì¼ ì „ì²´ ì½ê¸°
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()

    # 1. ìŠ¤ë§ˆíŠ¸ íŒŒì‹± ì‹œë„ (ë¹„ì •í˜• í…ìŠ¤íŠ¸, OCR ê²°ê³¼ ë“±)
    # parse_vocab_simpleì€ ì¤„ë°”ê¿ˆì´ ì—‰ë§ì¸ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ
    vocab_list = parse_vocab_simple(text)
    if vocab_list:
        return vocab_list

    # 2. ê¸°ì¡´ CSV íŒŒì‹± (ë°±ì—… - ìŠ¤ë§ˆíŠ¸ íŒŒì‹± ê²°ê³¼ê°€ ì—†ì„ ë•Œë§Œ ì‹¤í–‰)
    vocab_list = []
    lines = text.split('\n')

    for i, line in enumerate(lines):
        line = line.strip()
        if not line or line.startswith('#'):
            continue

        parts = line.split(',', 2)  # ìµœëŒ€ 3ê°œë¡œ ë¶„ë¦¬

        if len(parts) >= 3:
            # ë²ˆí˜¸,ë‹¨ì–´,ëœ»
            try:
                number = int(parts[0])
            except ValueError:
                number = i + 1
            word = parts[1].strip()
            meaning = parts[2].strip()
        elif len(parts) == 2:
            # ë‹¨ì–´,ëœ»
            number = i + 1
            word = parts[0].strip()
            meaning = parts[1].strip()
        else:
            continue

        if word and meaning:
            vocab_list.append(VocabItem(
                number=number,
                word=word,
                meaning=meaning
            ))

    return vocab_list


def extract_vocab_with_gemini_vision(image_path: str, api_key: str) -> List[VocabItem]:
    """
    Google Gemini Vision APIë¡œ ì´ë¯¸ì§€ì—ì„œ ë‹¨ì–´ ëª©ë¡ ì§ì ‘ ì¶”ì¶œ
    REST API ì§ì ‘ í˜¸ì¶œ ë°©ì‹ (SDK ë¬¸ì œ ìš°íšŒ)

    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        api_key: Google AI Studio API í‚¤

    Returns:
        VocabItem ë¦¬ìŠ¤íŠ¸
    """
    import json
    import base64
    import requests

    # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
    with open(image_path, 'rb') as f:
        image_data = base64.standard_b64encode(f.read()).decode('utf-8')

    # MIME íƒ€ì… ê²°ì •
    ext = image_path.lower().split('.')[-1]
    mime_type = {
        'jpg': 'image/jpeg',
        'jpeg': 'image/jpeg',
        'png': 'image/png',
        'gif': 'image/gif',
        'webp': 'image/webp'
    }.get(ext, 'image/jpeg')

    prompt = """ì´ ì´ë¯¸ì§€ëŠ” ì˜ì–´ ë‹¨ì–´ì¥ì…ë‹ˆë‹¤.
ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  ì˜ì–´ ë‹¨ì–´ì™€ í•œêµ­ì–´ ëœ»ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´):
[
  {"number": 301, "word": "heal", "meaning": "ì¹˜ë£Œí•˜ë‹¤"},
  {"number": 302, "word": "breath", "meaning": "ìˆ¨, í˜¸í¡"},
  ...
]

ì£¼ì˜ì‚¬í•­:
- ë²ˆí˜¸, ì˜ì–´ ë‹¨ì–´, í•œêµ­ì–´ ëœ»ì„ ì •í™•íˆ ì¶”ì¶œ
- í’ˆì‚¬(n. v. a. ë“±)ëŠ” ëœ»ì—ì„œ ì œì™¸
- ì´ë¯¸ì§€ê°€ íšŒì „ë˜ì–´ ìˆì–´ë„ ì˜¬ë°”ë¥´ê²Œ ì½ê¸°
- 2ë‹¨ìœ¼ë¡œ ëœ ê²½ìš° ì™¼ìª½ ë‹¨ë¶€í„° ìˆœì„œëŒ€ë¡œ
- JSONë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”"""

    # ì—¬ëŸ¬ ëª¨ë¸ ì‹œë„ (ìˆœì„œëŒ€ë¡œ)
    models_to_try = [
        'gemini-1.5-flash-latest',
        'gemini-1.5-pro-latest',
        'gemini-pro-vision',
    ]

    last_error = None

    for model_name in models_to_try:
        try:
            # REST API ì§ì ‘ í˜¸ì¶œ
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={api_key}"

            payload = {
                "contents": [{
                    "parts": [
                        {
                            "inlineData": {
                                "mimeType": mime_type,
                                "data": image_data
                            }
                        },
                        {
                            "text": prompt
                        }
                    ]
                }],
                "generationConfig": {
                    "temperature": 0.1,
                    "maxOutputTokens": 4096
                }
            }

            response = requests.post(url, json=payload, timeout=60)
            result = response.json()

            # ì—ëŸ¬ ì²´í¬
            if 'error' in result:
                error_msg = result['error'].get('message', str(result['error']))
                # 404 ì—ëŸ¬ë©´ ë‹¤ìŒ ëª¨ë¸ ì‹œë„
                if result['error'].get('code') == 404:
                    continue
                # 429 í• ë‹¹ëŸ‰ ì—ëŸ¬ë©´ ì˜ˆì™¸ ë°œìƒ
                raise Exception(f"{result['error'].get('code')} {result['error'].get('status')}. {result['error']}")

            # ì‘ë‹µ ì¶”ì¶œ
            if 'candidates' in result and result['candidates']:
                response_text = result['candidates'][0]['content']['parts'][0]['text'].strip()

                # JSON ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì•ˆì— ìˆì„ ìˆ˜ ìˆìŒ)
                if '```json' in response_text:
                    response_text = response_text.split('```json')[1].split('```')[0]
                elif '```' in response_text:
                    response_text = response_text.split('```')[1].split('```')[0]

                vocab_data = json.loads(response_text.strip())
                vocab_list = []
                for item in vocab_data:
                    vocab_list.append(VocabItem(
                        number=int(item.get('number', len(vocab_list) + 1)),
                        word=item.get('word', ''),
                        meaning=item.get('meaning', '')
                    ))
                return vocab_list

        except json.JSONDecodeError:
            continue
        except Exception as e:
            last_error = e
            # 404ê°€ ì•„ë‹Œ ì—ëŸ¬ë©´ ë°”ë¡œ ì˜ˆì™¸ ë°œìƒ
            if '404' not in str(e):
                raise

    # ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨
    if last_error:
        raise last_error
    return []


def extract_vocab_with_claude_vision(image_path: str, api_key: str) -> List[VocabItem]:
    """
    Claude Vision APIë¡œ ì´ë¯¸ì§€ì—ì„œ ë‹¨ì–´ ëª©ë¡ ì§ì ‘ ì¶”ì¶œ
    í‘œ í˜•ì‹ ë‹¨ì–´ì¥ì„ ì •í™•í•˜ê²Œ ì¸ì‹

    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        api_key: Anthropic API í‚¤

    Returns:
        VocabItem ë¦¬ìŠ¤íŠ¸
    """
    import base64
    import json

    # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
    with open(image_path, 'rb') as f:
        image_data = base64.standard_b64encode(f.read()).decode('utf-8')

    # íŒŒì¼ í™•ì¥ìë¡œ ë¯¸ë””ì–´ íƒ€ì… ê²°ì •
    ext = image_path.lower().split('.')[-1]
    media_type = {
        'jpg': 'image/jpeg',
        'jpeg': 'image/jpeg',
        'png': 'image/png',
        'gif': 'image/gif',
        'webp': 'image/webp'
    }.get(ext, 'image/jpeg')

    # Claude API í˜¸ì¶œ
    client = anthropic.Anthropic(api_key=api_key)

    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=4096,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": media_type,
                            "data": image_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": """ì´ ì´ë¯¸ì§€ëŠ” ì˜ì–´ ë‹¨ì–´ì¥ì…ë‹ˆë‹¤.
ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  ì˜ì–´ ë‹¨ì–´ì™€ í•œêµ­ì–´ ëœ»ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´):
[
  {"number": 301, "word": "heal", "meaning": "ì¹˜ë£Œí•˜ë‹¤"},
  {"number": 302, "word": "breath", "meaning": "ìˆ¨, í˜¸í¡"},
  ...
]

ì£¼ì˜ì‚¬í•­:
- ë²ˆí˜¸, ì˜ì–´ ë‹¨ì–´, í•œêµ­ì–´ ëœ»ì„ ì •í™•íˆ ì¶”ì¶œ
- í’ˆì‚¬(n. v. a. ë“±)ëŠ” ëœ»ì—ì„œ ì œì™¸
- ì´ë¯¸ì§€ê°€ íšŒì „ë˜ì–´ ìˆì–´ë„ ì˜¬ë°”ë¥´ê²Œ ì½ê¸°
- 2ë‹¨ìœ¼ë¡œ ëœ ê²½ìš° ì™¼ìª½ ë‹¨ë¶€í„° ìˆœì„œëŒ€ë¡œ"""
                    }
                ],
            }
        ],
    )

    # ì‘ë‹µ íŒŒì‹±
    response_text = message.content[0].text.strip()

    # JSON ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì•ˆì— ìˆì„ ìˆ˜ ìˆìŒ)
    if '```json' in response_text:
        response_text = response_text.split('```json')[1].split('```')[0]
    elif '```' in response_text:
        response_text = response_text.split('```')[1].split('```')[0]

    try:
        vocab_data = json.loads(response_text)
        vocab_list = []
        for item in vocab_data:
            vocab_list.append(VocabItem(
                number=int(item.get('number', len(vocab_list) + 1)),
                word=item.get('word', ''),
                meaning=item.get('meaning', '')
            ))
        return vocab_list
    except json.JSONDecodeError:
        # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        return []


def extract_text_with_cloud_vision(image_path: str, api_key: str = None) -> str:
    """
    Google Cloud Vision APIë¡œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    ë†’ì€ ì¸ì‹ë¥ , íŠ¹íˆ í•œê¸€+ì˜ì–´ í˜¼í•© í…ìŠ¤íŠ¸ì— ìš°ìˆ˜

    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        api_key: Google Cloud Vision API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ìŒ)

    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    import requests
    import base64

    with open(image_path, 'rb') as f:
        image_content = base64.b64encode(f.read()).decode('utf-8')

    if api_key:
        # API í‚¤ë¡œ ì§ì ‘ ì¸ì¦ (REST API ë°©ì‹)
        url = f"https://vision.googleapis.com/v1/images:annotate?key={api_key}"
        payload = {
            "requests": [{
                "image": {"content": image_content},
                "features": [{"type": "DOCUMENT_TEXT_DETECTION"}],
                "imageContext": {
                    "languageHints": ["ko", "en"]
                }
            }]
        }

        response = requests.post(url, json=payload)
        result = response.json()

        # ì—ëŸ¬ ì²´í¬
        if 'error' in result:
            raise Exception(f"Cloud Vision API ì˜¤ë¥˜: {result['error']}")

        if 'responses' in result and result['responses']:
            resp = result['responses'][0]
            if 'error' in resp:
                raise Exception(f"Cloud Vision API ì˜¤ë¥˜: {resp['error']}")

            # fullTextAnnotation ì‚¬ìš© (ë” ì •í™•í•œ ìˆœì„œ)
            full_text = resp.get('fullTextAnnotation', {}).get('text', '')
            if full_text:
                return full_text

            # fallback: textAnnotations
            annotations = resp.get('textAnnotations', [])
            if annotations:
                return annotations[0].get('description', '')
        return ""

    elif HAS_CLOUD_VISION:
        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦ (GOOGLE_APPLICATION_CREDENTIALS í™˜ê²½ë³€ìˆ˜)
        client = vision.ImageAnnotatorClient()

        with open(image_path, 'rb') as f:
            content = f.read()

        image = vision.Image(content=content)
        response = client.document_text_detection(image=image)

        if response.full_text_annotation:
            return response.full_text_annotation.text
        return ""

    else:
        raise ImportError("Google Cloud Visionì„ ì‚¬ìš©í•˜ë ¤ë©´ API í‚¤ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ google-cloud-visionì„ ì„¤ì¹˜í•˜ì„¸ìš”.")


def extract_text_from_image(image_path: str, two_column: bool = True,
                            use_cloud_vision: bool = True, api_key: str = None) -> str:
    """
    ì´ë¯¸ì§€ íŒŒì¼(JPG, PNG ë“±)ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)

    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        two_column: 2ë‹¨ ë¶„ë¦¬ ì²˜ë¦¬ ì—¬ë¶€ (pytesseract ì „ìš©)
        use_cloud_vision: Cloud Vision API ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        api_key: Google Cloud Vision API í‚¤

    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    # Cloud Vision ì‚¬ìš© ì‹œë„ (ê¶Œì¥)
    if use_cloud_vision and (HAS_CLOUD_VISION or api_key):
        try:
            text = extract_text_with_cloud_vision(image_path, api_key)
            if text:
                return text
        except Exception as e:
            print(f"Cloud Vision ì˜¤ë¥˜, pytesseractë¡œ fallback: {e}")

    # Fallback: pytesseract
    if not HAS_OCR:
        raise ImportError("OCR ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ pytesseractì™€ pillowë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")

    from PIL import Image, ImageEnhance

    img = Image.open(image_path)

    # ì „ì²˜ë¦¬ 1: í•´ìƒë„ í™•ëŒ€ (OCR ì¸ì‹ë¥  í–¥ìƒ)
    base_width = 2000
    if img.width < base_width:
        w_percent = (base_width / float(img.width))
        h_size = int((float(img.height) * float(w_percent)))
        img = img.resize((base_width, h_size), Image.Resampling.LANCZOS)

    # ë°©í–¥ ë³´ì •
    img = detect_and_fix_orientation(img)

    # ì „ì²˜ë¦¬ 2: ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë° í™”ì§ˆ ê°œì„ 
    img = img.convert('L')
    img = ImageEnhance.Contrast(img).enhance(2.0)
    img = ImageEnhance.Sharpness(img).enhance(1.5)

    full_text = ""

    if two_column:
        # 2ë‹¨ ë¶„ë¦¬ ì²˜ë¦¬ (ë‹¨ì–´ì¥ìš©)
        width, height = img.size
        for left, right in [(0, width // 2), (width // 2, width)]:
            crop_img = img.crop((left, 0, right, height))
            text = pytesseract.image_to_string(crop_img, lang='eng+kor', config='--psm 6')
            full_text += text + "\n"
    else:
        full_text = pytesseract.image_to_string(img, lang='eng+kor', config='--psm 6')

    return full_text


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    import sys
    if len(sys.argv) > 1:
        file_path = sys.argv[1]
        if file_path.endswith('.txt') or file_path.endswith('.csv'):
            vocab = load_vocab_from_text(file_path)
        else:
            vocab = extract_vocab_from_pdf(file_path)
        print(f"ì¶”ì¶œëœ ë‹¨ì–´ ìˆ˜: {len(vocab)}")
        for item in vocab[:5]:
            print(f"{item.number}. {item.word} - {item.meaning}")
